Appr.;Method;Year;Cite;Kp. Extraction;Kp. Extraction;Data Preparation;Data Preparation;Data Preparation;Data Preparation;Feature Extraction;Recognition;Evaluation;Evaluation;Remark;Remark
;;;;Frwk;No. Kp.;Clean.;Norm.;Sample Selection;Sample Selection;;Type;Method;Dataset;Data Split;
;;;;;;;;Sample Type;Label Scheme;;;;;;
Direct input;Serpa;2020;serpa2020evaluating;AlphaPose, OpenPose, PoseNet;15, 17, 18, 25;IFRM;-;F;F;direct kp.s as input;ML;MLP (No. of hidden layers varies from 0 to 16, with 128 neurons in each layer);URFD (Front);train/test: 75/25;Only consider fallen and no fall (No falling state)
Direct input;Ramirez;2021;ramirez2021fall;AlphaPose;17;IFRM;-;F;F;direct kp.s as input;ML;KNN, MLP, RF, SVM;UP-Fall (CAM1);10-fold cv with train/test: 70/30;
Direct input;Ramirez;2022;ramirez2022human;AlphaPose;17;IFRM;-;WD: \newline$W= 36$ \newline (2 sec), \newline $S = 1$, use only 3 frames in $W$;MOST;direct kp.s of 3 frames in a sample window (first, middle, and last frame);ML;RF, SVM, MLP, AdaBoost;UP-Fall (CAM1);10-fold cv with train/test: 70/30;
Hand-crafted;Alaoui;2019;alaouiFallDetectionElderly2019;Custom;15;ConfRM;-;V;V;kp. angles and kp. distances, followed by PCA;ML;DT, KNN,  RF, SVM;Le2i, URFD;-;
Hand-crafted;Chen;2020;chen2020fall;OpenPose;15;-;-;-;-;speed of descent at center of hip joints (SDCH), angle between center line and ground (ACLG), with-to-height ratio (WHR);R;A decisive process based on thresholds;Custom \newline (10 subjects - 60 fall/40 non-fall videos);-;
Hand-crafted;Wang;2020;wang2020fall;OpenPose;18;-;-;WD:\newline $W=20$, \newline $S=1$;-;Centroid Drop Rate (CDR), Upper Limb Velocity (ULV), 04 parameters of body enclosing external ellipse (PEE);ML $+$ R;Channel 1: MLP, Channel 2: RF, then Rules;Le2i,URFD;10-fold cv;
Hand-crafted;Dentamaro;2021;dentamaro2021fall;OpenPose;15;MLI;-;V;V;spatio-temporal features , sigma-lognormal features, then statistical measures;ML;KNN, SVM with Gaussian Kernel (SVC), RF;URFD, Le2i;10-fold cv;
Hand-crafted;Liu;2021;liu2021automatic;OpenPose;25;UKpRM;-;F;F;spine ratio, height-to-width ratio (HWR) , distance features, acceleration features, deflection features;ML;KNN, SVM, Boosting;Custom \newline (240 fall/1267 ADL);train/test: 80/20;
Hand-crafted;Zhang;2022;zhang2022visual;OpenPose;25;-;-;-;-;HWR, angle of human spine inclination $\theta_1$, angle of right knee inclination $\theta_2$, angle of left knee inclination $\theta_3$;Rule;threshold-based;URFD, Self-built;train/val\newline/test: 50/25/15;
DL;Hasan;2019;hasanRobustPoseBasedHuman2019;OpenPose;25;UKpRM;MidHip Norm.;WD:\newline  $W=24$, \newline $S=8$;ALO;stacked 2-layer LSTM;ML;Softmax;MultiCam, URFD;train/test: 80/20;Whole video splited into 24-frames clips, any fall clip detected then video fall
DL;Jeong;2019;jeongHumanskeletonBasedFallDetection2019;OpenPose;15;MLI;VRes Norm.;WD:\newline $W=10$, \newline $S=1$;MOST;Human Center Line Coordinate (HCLC) + Speed of HCLC (SHCLC) fed into a stacked 2-layer LSTM;Rule;Rule base on classification of LSTM ;URFD;-;
DL;Lin;2020;lin2020framework;OpenPose;25;MLI, UKpRM, ConfRM;Min-Max Norm., RP-Norm.;WD:\newline $W=100$;-;RNN, or LSTM, or GRU (1 layer);ML;Softmax;Mixed dataset (URFD + Fall detection \cite{falldetdataset});train/test: 80/20;Dataset contains a total of 1140 groups, each group 100 frames
DL;Chang;2021;chang2021pose;OpenPose, OpenPose-Light;18;-;-;WD: \newline $W=16$;-;LSTM;ML;Softmax;MultiCam, Le2i, URFD;-;Pre-filtering reduces detection model runtime by filtering non-fall events.
DL;Galvao;2021;galvao2021framework;OpenPose;18;-;-;-;-;ST-GCN features => Autoencoder => Reconstruction Error (RE);Rule;Compare RE with a calculated threshold;URFD, UP-Fall;train/valid\newline/test: 95/5/5. Train \& valid  sets consist only ADL data;Test on: single dataset, cross-dataset, and joint dataset
DL;Huu;2021;huu2021proposing;OpenPose;18;IFRM;-;WD;-;LSTM;ML;Softmax;Mixed dataset (Self-built + MPII \cite{mpiiAndriluka20142d});varied depend on label;Labels: lying, crouching, sitting, standing, walking, punching, and falling
DL;Chen;2022;chen2022video;LPN \cite{zhang2019simple}, Pose Lifting \cite{martinez2017simple};25;-;BP-Norm.;WD: \newline $W=300$;-;1D CNN (Dilated Convolution + Residual Connection);ML;1D Conv. (Output size = 2);NTU RGB + D \cite{shahroudy2016ntu};-;The dataset has nearly  57,000 videos (60 action classes), including falls
DL;Inturi;2022;inturi2022novel;AlphaPose;17;IFRM;-;WD:\newline $W=64$, \newline $S=48$;-;CNN + LSTM;ML;Softmax;UP-Fall;-;
DL;Juraev;2022;juraev2022exploring;OpenPose, AlphaPose, OpenPifpaf, MoveNet, DCPose;17, 18;-;Min-Max Norm.;WD: \newline $W=25$;-;LSTM, or Transformer-based model;ML;MLP;Mixed Dataset\newline(AIHub, Synthetic generated from KIST SynADL \cite{hwang2020eldersim};train/test: 75/25;Labels: falling down, standing up, lying down, walking
DL;Lau;2022;lau2022fall; MediaPipe Pose;33 (2D \& 3D);IFRM;UKpRM (2D kp), R.Norm. (3D kp);WD:\newline $W=35$;-;2D: gait speed, with-to-height ratio (WHR) + 3D: trunk angle => 1D CNN, (Attention-based) LSTM/GRU;ML;Softmax;Mixed dataset (MultiCam, URFD, and Fall detection dataset \cite{adhikari2017activity});train/val\newline/test: 70/15/15;Labels: non-fall, pre-impact fall (before hitting the floor), and fall (after hitting the floor)
DL;Li;2022;li2022kamtfenet;YoloV3 \cite{redmon2018yolov3}, MSPN \cite{mspnli2019rethinking};17;ConfRM;-;WD:\newline  $W=30$;label of last frame;Adaptive keypoint attention module and  a stacked 3-layer RLSTM  (A modifed version of LSTM using residual connection \cite{resnet});ML;MLP;URFD, Le2i, Self-built;train/test: 70/30;
DL;Suarez;2022;suarez2022afar;MediaPipe Pose;33 (3D);-;MidHip Norm. + Value scaling;WD:\newline $W=16$;-;keypoints, distance features, velocity features => 1D CNN;ML;Softmax;URFD, UP-Fall;URFD: train/test = 80/20. UP-Fall (CAM1): 2 first trials to train, the last for test.;UP-Fall (CAM1) has 3 action trials/subject 
DL;Yadav;2022;yadav2022arfdnet;OpenPose;25;ConfRM;Min-Max Norm.;WD:\newline  $W=45$, $S=9$;-;1D CNN + GRU;ML;Softmax;UP-Fall, Self-built;train/valid\newline/test: 70/15/15;
DL;Yuan;2022;yuan2022real;AlphaPose;17;-;VRes Norm.;WD:\newline $W=20$;;kp. values + kp. velocities => GCN + self-attention (Spatial + Temporal);ML;Softmax;Le2i;-;
DL;Zahan;2022;zahan2022sdfa;OpenPose;25;IFRM;Zero-mean and unit variance;WD:\newline $W=300$ ($W=1145$ for UP-Fall dataset);-;Spatial Graph Convolutional Network (SGCN) + Separable Temporal Convolutional Network (Sep-TCN);ML;Softmax;URFD, UP-Fall;URFD: train/test = 70/30. Up-Fall: 5-fold cv (Cross-Fall and Cross-Trial);Random masking during training: spatial joints masking and temporal frames masking
DL;Amsaprabhaa;2023;amsaprabhaa2023multimodal;OpenPose;25;UKpRM, MLI;Min-Max Norm., RP-Norm.;WD;-;ST-GCN \cite{yan2018spatial} + 1D CNN;ML;Softmax;URFD, Self-built (190 fall/210 non-fall videos);train/test: 70/30;
DL;Ramirez;2023;ramirez2023bert;AlphaPose;17;IFRM;-;WD: $W= 36$ \newline (2 sec), \newline $S = 1$, use only 3 frames in $W$;MOST;12-layer BERT (Base version) \cite{devlin2018bert};ML;Softmax;UP-Fall (CAM1);train/val\newline/test: 70/15/15;Multi-class: 11 labels (5 falls, 6 ADLs) from Up-Fall dataset and an unknown label.
